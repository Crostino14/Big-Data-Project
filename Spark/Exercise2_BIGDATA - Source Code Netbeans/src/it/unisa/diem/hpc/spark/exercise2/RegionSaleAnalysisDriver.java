/*
 * Course: High Performance Computing 2023/2024
 *
 * Lecturer: Giuseppe D'Aniello      gidaniello@unisa.it
 *
 * Student and Creator:
 * Agostino Cardamone       0622702276      a.cardamone7@studenti.unisa.it
 *
 * Source Code of Spark Exercise 2 Solution.
 *
 *                        ASSIGNMENT:
 *
 * This Java code is designed to analyze a dataset containing sales information from a store, 
 * providing details about the sold products. The dataset encompasses various fields, 
 * including order details, customer information, and product specifics. 
 * Below is an overview of the available fields:
 * - Order ID: A unique identifier for each order.
 * - Customer ID: A unique identifier for each customer.
 * - Order Date: The date of the order placement.
 * - Ship Date: The date the order was shipped.
 * - Ship Mode: The shipping mode for the order (e.g., standard, same-day).
 * - Segment: The customer segment (e.g., Consumer, Corporate, Home Office).
 * - Region: The region where the customer is located (e.g., West, Central, East).
 * - Category: The category of the product purchased (e.g., Furniture, Technology, Office Supplies).
 * - Sub-Category: The sub-category of the product purchased (e.g., Chairs, Desktops, Paper).
 * - Product Name: The name of the product purchased.
 * - Sales: The sales revenue for the product purchased.
 * - Quantity: The number of units of the product purchased.
 * - Discount: The discount applied to the product purchased.
 * - Profit: The profit generated by the product purchased.
 *
 * Exercise 1 – Map Reduce:
 * Implement a Hadoop Map Reduce program to analyze the dataset in a different manner from the second exercise. This could involve generating statistics, creating rankings, calculating indices, or any unique approach.
 *
 * Exercise 2 – Spark:
 * Determine which region has the highest sales and which region has the lowest sales using Spark. This analysis aims to provide insights into regional sales performance within the dataset.
 *
 */
package it.unisa.diem.hpc.spark.exercise2;

import java.io.Serializable;
import java.util.Arrays;
import java.util.Comparator;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import scala.Tuple2;

/**
 *
 * author agost
 * 
 * RegionSaleAnalysisDriver is the main driver for region sales analysis.
 * It accepts input and output paths as command-line arguments.
 * 
 */
public class RegionSaleAnalysisDriver {

    /**
     * The main method is the entry point of the program.
     *
     * @param args [0] = input path, [1] = output path
     */
    public static void main(String[] args) {
        // Check for correct syntax of input parameters
        if (args.length != 2) {
            System.err.println("Usage: RegionSaleAnalysisDriver <input path> <output path>");
            System.exit(-1);
        }

        // Set input and output paths
        String inputPath = args[0];
        String outputPath = args[1];

        // Configure Spark
        SparkConf conf = new SparkConf().setAppName("Region Sales Analysis");
        JavaSparkContext sc = new JavaSparkContext(conf);

        // Read lines from the input file
        JavaRDD<String> lines = sc.textFile(inputPath);

        // Dynamically get the indices of "Region" and "Sales"
        String firstLine = lines.first();
        String[] headers = firstLine.split(";");
        int regionIndex = Arrays.asList(headers).indexOf("Region");
        int salesIndex = Arrays.asList(headers).indexOf("Sales");

        // Create a pair (region, sales) for each line in the file, excluding the header
        JavaPairRDD<String, Double> salesByRegion = lines
                .filter(line -> !line.equals(firstLine))
                .mapToPair(line -> {
                    String[] parts = line.split(";");
                    String region = parts[regionIndex];
                    double sales = Double.parseDouble(parts[salesIndex]);
                    return new Tuple2<>(region, sales);
                })
                .reduceByKey(Double::sum);

        // Find the region with the maximum and minimum sales
        Tuple2<String, Double> maxSalesRegion = salesByRegion.max((Comparator<Tuple2<String, Double>> & Serializable)
                (t1, t2) -> t1._2.compareTo(t2._2));
        Tuple2<String, Double> minSalesRegion = salesByRegion.min((Comparator<Tuple2<String, Double>> & Serializable)
                (t1, t2) -> t1._2.compareTo(t2._2));

        // Save the results as text in two separate directories
        sc.parallelize(Arrays.asList("Max Sales Region: " + maxSalesRegion), 1).saveAsTextFile(outputPath + "/max_sales");
        sc.parallelize(Arrays.asList("Min Sales Region: " + minSalesRegion), 1).saveAsTextFile(outputPath + "/min_sales");

        // Close the Spark context
        sc.close();
    }
}