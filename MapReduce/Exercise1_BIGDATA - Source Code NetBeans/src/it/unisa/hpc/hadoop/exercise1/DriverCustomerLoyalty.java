/**
 * Course: High Performance Computing 2023/2024
 *
 * Lecturer: Giuseppe D'Aniello      gidaniello@unisa.it
 *
 * Student and Creator:
 * Agostino Cardamone       0622702276      a.cardamone7@studenti.unisa.it
 *
 * Source Code of Hadoop MapReduce Exercise 1 Solution.
 *
 *                        ASSIGNMENT:
 * 
 * This Java code is designed to analyze a dataset containing sales information from a store, 
 * providing details about the sold products. The dataset encompasses various fields, 
 * including order details, customer information, and product specifics. 
 * Below is an overview of the available fields:
 * - Order ID: A unique identifier for each order.
 * - Customer ID: A unique identifier for each customer.
 * - Order Date: The date of the order placement.
 * - Ship Date: The date the order was shipped.
 * - Ship Mode: The shipping mode for the order (e.g., standard, same-day).
 * - Segment: The customer segment (e.g., Consumer, Corporate, Home Office).
 * - Region: The region where the customer is located (e.g., West, Central, East).
 * - Category: The category of the product purchased (e.g., Furniture, Technology, Office Supplies).
 * - Sub-Category: The sub-category of the product purchased (e.g., Chairs, Desktops, Paper).
 * - Product Name: The name of the product purchased.
 * - Sales: The sales revenue for the product purchased.
 * - Quantity: The number of units of the product purchased.
 * - Discount: The discount applied to the product purchased.
 * - Profit: The profit generated by the product purchased.
 *
 * Exercise 1 – Map Reduce:
 * Implement a Hadoop Map Reduce program to analyze the dataset in a different manner from the second exercise. 
 * This could involve generating statistics, creating rankings, calculating indices, or any unique approach.
 *
 * Exercise 2 – Spark:
 * Determine which region has the highest sales and which region has the lowest sales using Spark. 
 * This analysis aims to provide insights into regional sales performance within the dataset.
 *
 */
package it.unisa.hpc.hadoop.exercise1;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

/**
 * The driver class for executing the Hadoop MapReduce job for customer loyalty analysis.
 * It configures and runs the MapReduce job.
 *
 * @author agost
 */
public class DriverCustomerLoyalty {

    /**
     * The main method is the entry point of the program.
     *
     * @param args Command line arguments: [0] = input path, [1] = output path
     * @throws Exception If an error occurs during job execution.
     */
    public static void main(String[] args) throws Exception {
        // Verify the correct syntax of input parameters
        if (args.length != 2) {
            System.err.println("Usage: DriverCustomerLoyalty <input path> <output path>");
            System.exit(-1);
        }

        // Configure Hadoop job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Customer Loyalty Analysis");
        job.setJarByClass(DriverCustomerLoyalty.class);
        job.setMapperClass(MapperCustomerLoyalty.class);
        job.setReducerClass(ReducerCustomerLoyalty.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(CustomerDataWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(NullWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        // Execute the job and exit
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}