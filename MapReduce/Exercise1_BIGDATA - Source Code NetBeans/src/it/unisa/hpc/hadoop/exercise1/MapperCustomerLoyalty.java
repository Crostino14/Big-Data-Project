/**
 * Course: High Performance Computing 2023/2024
 *
 * Lecturer: Giuseppe D'Aniello      gidaniello@unisa.it
 *
 * Student and Creator:
 * Agostino Cardamone       0622702276      a.cardamone7@studenti.unisa.it
 *
 * Source Code of Hadoop MapReduce Exercise 1 Solution.
 *
 *                        ASSIGNMENT:
 * 
 * This Java code is designed to analyze a dataset containing sales information from a store, 
 * providing details about the sold products. The dataset encompasses various fields, 
 * including order details, customer information, and product specifics. 
 * Below is an overview of the available fields:
 * - Order ID: A unique identifier for each order.
 * - Customer ID: A unique identifier for each customer.
 * - Order Date: The date of the order placement.
 * - Ship Date: The date the order was shipped.
 * - Ship Mode: The shipping mode for the order (e.g., standard, same-day).
 * - Segment: The customer segment (e.g., Consumer, Corporate, Home Office).
 * - Region: The region where the customer is located (e.g., West, Central, East).
 * - Category: The category of the product purchased (e.g., Furniture, Technology, Office Supplies).
 * - Sub-Category: The sub-category of the product purchased (e.g., Chairs, Desktops, Paper).
 * - Product Name: The name of the product purchased.
 * - Sales: The sales revenue for the product purchased.
 * - Quantity: The number of units of the product purchased.
 * - Discount: The discount applied to the product purchased.
 * - Profit: The profit generated by the product purchased.
 *
 * Exercise 1 – Map Reduce:
 * Implement a Hadoop Map Reduce program to analyze the dataset in a different manner from the second exercise. 
 * This could involve generating statistics, creating rankings, calculating indices, or any unique approach.
 *
 * Exercise 2 – Spark:
 * Determine which region has the highest sales and which region has the lowest sales using Spark. 
 * This analysis aims to provide insights into regional sales performance within the dataset.
 *
 */
package it.unisa.hpc.hadoop.exercise1;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

/**
 * Mapper class for processing input records in Hadoop MapReduce job for customer loyalty analysis.
 * It extracts relevant fields and emits key-value pairs.
 *
 * @author agost
 */
public class MapperCustomerLoyalty extends Mapper<LongWritable, Text, Text, CustomerDataWritable> {
    private Map<String, Integer> columnIndexMap = new HashMap<>();
    private boolean isHeader = true;

    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        columnIndexMap = new HashMap<>();
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        // Split the input line into fields
        String[] fields = value.toString().split(";");

        // Skip the header line and setup the column index mapping
        if (isHeader) {
            for (int i = 0; i < fields.length; i++) {
                columnIndexMap.put(fields[i].trim(), i);
            }
            isHeader = false;
            return;
        }

        // Extract fields for each record
        String customerID = fields[columnIndexMap.get("Customer ID")];
        double sales = Double.parseDouble(fields[columnIndexMap.get("Sales")]);
        String orderDate = fields[columnIndexMap.get("Order Date")];
        String customerSegment = fields[columnIndexMap.get("Customer Segment")];

        // Create a new CustomerDataWritable object
        CustomerDataWritable customerData = new CustomerDataWritable(orderDate, sales, customerSegment);

        // Write the output key-value pair
        context.write(new Text(customerID), customerData);
    }
}