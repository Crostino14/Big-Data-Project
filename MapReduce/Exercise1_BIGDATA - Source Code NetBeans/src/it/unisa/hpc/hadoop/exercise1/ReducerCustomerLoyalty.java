/**
 * Course: High Performance Computing 2023/2024
 *
 * Lecturer: Giuseppe D'Aniello      gidaniello@unisa.it
 *
 * Student and Creator:
 * Agostino Cardamone       0622702276      a.cardamone7@studenti.unisa.it
 *
 * Source Code of Hadoop MapReduce Exercise 1 Solution.
 *
 *                        ASSIGNMENT:
 * 
 * This Java code is designed to analyze a dataset containing sales information from a store, 
 * providing details about the sold products. The dataset encompasses various fields, 
 * including order details, customer information, and product specifics. 
 * Below is an overview of the available fields:
 * - Order ID: A unique identifier for each order.
 * - Customer ID: A unique identifier for each customer.
 * - Order Date: The date of the order placement.
 * - Ship Date: The date the order was shipped.
 * - Ship Mode: The shipping mode for the order (e.g., standard, same-day).
 * - Segment: The customer segment (e.g., Consumer, Corporate, Home Office).
 * - Region: The region where the customer is located (e.g., West, Central, East).
 * - Category: The category of the product purchased (e.g., Furniture, Technology, Office Supplies).
 * - Sub-Category: The sub-category of the product purchased (e.g., Chairs, Desktops, Paper).
 * - Product Name: The name of the product purchased.
 * - Sales: The sales revenue for the product purchased.
 * - Quantity: The number of units of the product purchased.
 * - Discount: The discount applied to the product purchased.
 * - Profit: The profit generated by the product purchased.
 *
 * Exercise 1 – Map Reduce:
 * Implement a Hadoop Map Reduce program to analyze the dataset in a different manner from the second exercise. 
 * This could involve generating statistics, creating rankings, calculating indices, or any unique approach.
 *
 * Exercise 2 – Spark:
 * Determine which region has the highest sales and which region has the lowest sales using Spark. 
 * This analysis aims to provide insights into regional sales performance within the dataset.
 *
 */
package it.unisa.hpc.hadoop.exercise1;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.*;


/**
 *
 * @author agost
 */

public class ReducerCustomerLoyalty extends Reducer<Text, CustomerDataWritable, Text, NullWritable> {
    private SimpleDateFormat dateFormat = new SimpleDateFormat("dd/MM/yyyy");
    private boolean isHeaderWritten = false;
    private Calendar calendar = Calendar.getInstance();

    @Override
    protected void reduce(Text key, Iterable<CustomerDataWritable> values, Context context) 
            throws IOException, InterruptedException {
        if (!isHeaderWritten) {
            context.write(new Text("CustomerID;CustomerSegments;OrderCount;TotalSales;NumberOfPurchasesCurrentYear"), NullWritable.get());
            isHeaderWritten = true;
        }

        double totalSales = 0.0;
        int numberOfPurchasesCurrentYear = 0;
        Set<String> customerSegments = new HashSet<>();
        int currentYear = -1;

        for (CustomerDataWritable value : values) {
            totalSales += value.getSales().get();
            customerSegments.add(value.getCustomerSegment().toString());
            
            Date orderDate;
            try {
                orderDate = dateFormat.parse(value.getOrderDate().toString());
                calendar.setTime(orderDate);
                int orderYear = calendar.get(Calendar.YEAR);

                currentYear = 2015;
                
                if (orderYear == currentYear) {
                    numberOfPurchasesCurrentYear++;
                }
            } catch (ParseException e) {
                // Log and continue without adding this date
                context.getCounter("CustomerLoyaltyReducer", "InvalidDates").increment(1);
            }
        }

        String output = String.format("%s;%s;%d;%.2f;%d", 
                                      key.toString(), 
                                      String.join(", ", customerSegments), 
                                      customerSegments.size(), 
                                      totalSales, 
                                      numberOfPurchasesCurrentYear);

        context.write(new Text(output), NullWritable.get());
    }
}


